{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lead Scoring: Missing Values Handling\n",
        "\n",
        "This notebook downloads the dataset, inspects missing values, and imputes them:\n",
        "- Categorical features → 'NA'\n",
        "- Numerical features → 0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lead_source</th>\n",
              "      <th>industry</th>\n",
              "      <th>number_of_courses_viewed</th>\n",
              "      <th>annual_income</th>\n",
              "      <th>employment_status</th>\n",
              "      <th>location</th>\n",
              "      <th>interaction_count</th>\n",
              "      <th>lead_score</th>\n",
              "      <th>converted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>paid_ads</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>79450.0</td>\n",
              "      <td>unemployed</td>\n",
              "      <td>south_america</td>\n",
              "      <td>4</td>\n",
              "      <td>0.94</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>social_media</td>\n",
              "      <td>retail</td>\n",
              "      <td>1</td>\n",
              "      <td>46992.0</td>\n",
              "      <td>employed</td>\n",
              "      <td>south_america</td>\n",
              "      <td>1</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>events</td>\n",
              "      <td>healthcare</td>\n",
              "      <td>5</td>\n",
              "      <td>78796.0</td>\n",
              "      <td>unemployed</td>\n",
              "      <td>australia</td>\n",
              "      <td>3</td>\n",
              "      <td>0.69</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>paid_ads</td>\n",
              "      <td>retail</td>\n",
              "      <td>2</td>\n",
              "      <td>83843.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>australia</td>\n",
              "      <td>1</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>referral</td>\n",
              "      <td>education</td>\n",
              "      <td>3</td>\n",
              "      <td>85012.0</td>\n",
              "      <td>self_employed</td>\n",
              "      <td>europe</td>\n",
              "      <td>3</td>\n",
              "      <td>0.62</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    lead_source    industry  number_of_courses_viewed  annual_income  \\\n",
              "0      paid_ads         NaN                         1        79450.0   \n",
              "1  social_media      retail                         1        46992.0   \n",
              "2        events  healthcare                         5        78796.0   \n",
              "3      paid_ads      retail                         2        83843.0   \n",
              "4      referral   education                         3        85012.0   \n",
              "\n",
              "  employment_status       location  interaction_count  lead_score  converted  \n",
              "0        unemployed  south_america                  4        0.94          1  \n",
              "1          employed  south_america                  1        0.80          0  \n",
              "2        unemployed      australia                  3        0.69          1  \n",
              "3               NaN      australia                  1        0.87          0  \n",
              "4     self_employed         europe                  3        0.62          1  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download data\n",
        "url = 'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/course_lead_scoring.csv'\n",
        "!wget -q $url -O course_lead_scoring.csv\n",
        "\n",
        "# Load\n",
        "df = pd.read_csv('course_lead_scoring.csv')\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns with missing values: {'annual_income': 181, 'industry': 134, 'lead_source': 128, 'employment_status': 100, 'location': 63}\n",
            "Total missing: 606\n"
          ]
        }
      ],
      "source": [
        "# Missing values overview\n",
        "missing_counts = df.isna().sum().sort_values(ascending=False)\n",
        "print('Columns with missing values:', missing_counts[missing_counts > 0].to_dict())\n",
        "print('Total missing:', int(missing_counts.sum()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total missing after imputation: 0\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lead_source</th>\n",
              "      <th>industry</th>\n",
              "      <th>number_of_courses_viewed</th>\n",
              "      <th>annual_income</th>\n",
              "      <th>employment_status</th>\n",
              "      <th>location</th>\n",
              "      <th>interaction_count</th>\n",
              "      <th>lead_score</th>\n",
              "      <th>converted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>paid_ads</td>\n",
              "      <td>NA</td>\n",
              "      <td>1</td>\n",
              "      <td>79450.0</td>\n",
              "      <td>unemployed</td>\n",
              "      <td>south_america</td>\n",
              "      <td>4</td>\n",
              "      <td>0.94</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>social_media</td>\n",
              "      <td>retail</td>\n",
              "      <td>1</td>\n",
              "      <td>46992.0</td>\n",
              "      <td>employed</td>\n",
              "      <td>south_america</td>\n",
              "      <td>1</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>events</td>\n",
              "      <td>healthcare</td>\n",
              "      <td>5</td>\n",
              "      <td>78796.0</td>\n",
              "      <td>unemployed</td>\n",
              "      <td>australia</td>\n",
              "      <td>3</td>\n",
              "      <td>0.69</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>paid_ads</td>\n",
              "      <td>retail</td>\n",
              "      <td>2</td>\n",
              "      <td>83843.0</td>\n",
              "      <td>NA</td>\n",
              "      <td>australia</td>\n",
              "      <td>1</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>referral</td>\n",
              "      <td>education</td>\n",
              "      <td>3</td>\n",
              "      <td>85012.0</td>\n",
              "      <td>self_employed</td>\n",
              "      <td>europe</td>\n",
              "      <td>3</td>\n",
              "      <td>0.62</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    lead_source    industry  number_of_courses_viewed  annual_income  \\\n",
              "0      paid_ads          NA                         1        79450.0   \n",
              "1  social_media      retail                         1        46992.0   \n",
              "2        events  healthcare                         5        78796.0   \n",
              "3      paid_ads      retail                         2        83843.0   \n",
              "4      referral   education                         3        85012.0   \n",
              "\n",
              "  employment_status       location  interaction_count  lead_score  converted  \n",
              "0        unemployed  south_america                  4        0.94          1  \n",
              "1          employed  south_america                  1        0.80          0  \n",
              "2        unemployed      australia                  3        0.69          1  \n",
              "3                NA      australia                  1        0.87          0  \n",
              "4     self_employed         europe                  3        0.62          1  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Impute: categorical -> 'NA', numerical -> 0.0\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Replace\n",
        "if categorical_cols:\n",
        "    df[categorical_cols] = df[categorical_cols].fillna('NA')\n",
        "if numeric_cols:\n",
        "    df[numeric_cols] = df[numeric_cols].fillna(0.0)\n",
        "\n",
        "# Verify\n",
        "missing_after = df.isna().sum().sum()\n",
        "print('Total missing after imputation:', int(missing_after))\n",
        "\n",
        "# Show sample\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mode for industry: retail\n"
          ]
        }
      ],
      "source": [
        "# Mode of 'industry'\n",
        "if 'industry' in df.columns:\n",
        "    mode_vals = df['industry'].mode(dropna=True)\n",
        "    if len(mode_vals) > 0:\n",
        "        print('Mode for industry:', str(mode_vals.iloc[0]))\n",
        "    else:\n",
        "        print('No mode for industry (empty after dropna).')\n",
        "else:\n",
        "    print(\"Column 'industry' not found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top correlated pair: number_of_courses_viewed  number_of_courses_viewed    0.000000\n",
            "                          annual_income               0.009770\n",
            "                          interaction_count           0.023565\n",
            "                          lead_score                  0.004879\n",
            "                          converted                   0.435914\n",
            "annual_income             number_of_courses_viewed    0.009770\n",
            "                          annual_income               0.000000\n",
            "                          interaction_count           0.027036\n",
            "                          lead_score                  0.015610\n",
            "                          converted                   0.053131\n",
            "interaction_count         number_of_courses_viewed    0.023565\n",
            "                          annual_income               0.027036\n",
            "                          interaction_count           0.000000\n",
            "                          lead_score                  0.009888\n",
            "                          converted                   0.374573\n",
            "lead_score                number_of_courses_viewed    0.004879\n",
            "                          annual_income               0.015610\n",
            "                          interaction_count           0.009888\n",
            "                          lead_score                  0.000000\n",
            "                          converted                   0.193673\n",
            "converted                 number_of_courses_viewed    0.435914\n",
            "                          annual_income               0.053131\n",
            "                          interaction_count           0.374573\n",
            "                          lead_score                  0.193673\n",
            "                          converted                   0.000000\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Correlation matrix (numeric features) and strongest pair\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "if len(numeric_cols) < 2:\n",
        "    print('Not enough numeric features to compute correlations.')\n",
        "else:\n",
        "    corr = df[numeric_cols].corr(method='pearson')\n",
        "    # Find the pair with the largest absolute correlation (exclude diagonal)\n",
        "    corr_abs = corr.abs()\n",
        "    np.fill_diagonal(corr_abs.values, 0.0)\n",
        "    max_idx = corr_abs.stack().idxmax()\n",
        "    max_val = corr_abs.loc[max_idx]\n",
        "    print('Top correlated pair:', corr_abs.stack())\n",
        "    #print('Correlation:', corr.loc[max_idx], 4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature_a</th>\n",
              "      <th>feature_b</th>\n",
              "      <th>pearson_corr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>interaction_count</td>\n",
              "      <td>lead_score</td>\n",
              "      <td>0.0099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>number_of_courses_viewed</td>\n",
              "      <td>lead_score</td>\n",
              "      <td>-0.0049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>number_of_courses_viewed</td>\n",
              "      <td>interaction_count</td>\n",
              "      <td>-0.0236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>annual_income</td>\n",
              "      <td>interaction_count</td>\n",
              "      <td>0.0270</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  feature_a          feature_b  pearson_corr\n",
              "0         interaction_count         lead_score        0.0099\n",
              "1  number_of_courses_viewed         lead_score       -0.0049\n",
              "2  number_of_courses_viewed  interaction_count       -0.0236\n",
              "3             annual_income  interaction_count        0.0270"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Correlation table for selected pairs\n",
        "pairs = [\n",
        "    ('interaction_count', 'lead_score'),\n",
        "    ('number_of_courses_viewed', 'lead_score'),\n",
        "    ('number_of_courses_viewed', 'interaction_count'),\n",
        "    ('annual_income', 'interaction_count'),\n",
        "]\n",
        "\n",
        "# ensure columns exist and are numeric for correlation\n",
        "present_pairs = []\n",
        "for a, b in pairs:\n",
        "    if a in df.columns and b in df.columns:\n",
        "        a_vals = pd.to_numeric(df[a], errors='coerce')\n",
        "        b_vals = pd.to_numeric(df[b], errors='coerce')\n",
        "        corr = a_vals.corr(b_vals, method='pearson')\n",
        "        present_pairs.append({'feature_a': a, 'feature_b': b, 'pearson_corr': round(float(corr), 4)})\n",
        "    else:\n",
        "        present_pairs.append({'feature_a': a, 'feature_b': b, 'pearson_corr': None})\n",
        "\n",
        "corr_table = pd.DataFrame(present_pairs)\n",
        "corr_table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train': 877, 'val': 292, 'test': 293, 'target': 'converted'}\n"
          ]
        }
      ],
      "source": [
        "# Split data: 60/20/20 with seed=42, ensure y not in X\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Heuristic to pick target column (adjust if needed)\n",
        "candidate_targets = ['lead_status', 'converted', 'is_enrolled', 'label', 'target']\n",
        "for t in candidate_targets:\n",
        "    if t in df.columns:\n",
        "        y_col = t\n",
        "        break\n",
        "else:\n",
        "    raise ValueError('Target column not found. Please set y_col to the correct target column name.')\n",
        "\n",
        "# Features without target\n",
        "y = df[y_col]\n",
        "X = df.drop(columns=[y_col]).copy()\n",
        "\n",
        "# First split: train vs temp (val+test)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.4, random_state=42, stratify=y if y.nunique() > 1 else None\n",
        ")\n",
        "\n",
        "# Second split: val vs test (50/50 of temp)\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp if y_temp.nunique() > 1 else None\n",
        ")\n",
        "\n",
        "print({\n",
        "    'train': len(X_train),\n",
        "    'val': len(X_val),\n",
        "    'test': len(X_test),\n",
        "    'target': y_col,\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MI scores (train): {'industry': 0.01, 'location': 0.0, 'lead_source': 0.03, 'employment_status': 0.01}\n",
            "Best categorical by MI: lead_source\n"
          ]
        }
      ],
      "source": [
        "# Mutual information for selected categorical variables (train only)\n",
        "from sklearn.metrics import mutual_info_score\n",
        "\n",
        "candidates = ['industry', 'location', 'lead_source', 'employment_status']\n",
        "\n",
        "mi_scores = {}\n",
        "for col in candidates:\n",
        "    if col in X_train.columns:\n",
        "        x = X_train[col].astype(str)\n",
        "        mi = mutual_info_score(x, y_train)\n",
        "        mi_scores[col] = round(float(mi), 2)\n",
        "    else:\n",
        "        mi_scores[col] = None\n",
        "\n",
        "print('MI scores (train):', mi_scores)\n",
        "best = max((k for k in mi_scores if mi_scores[k] is not None), key=lambda k: mi_scores[k])\n",
        "print('Best categorical by MI:', best)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy: 0.68\n"
          ]
        }
      ],
      "source": [
        "# Logistic Regression with One-Hot Encoding; report validation accuracy\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Ensure splits exist\n",
        "assert 'X_train' in globals() and 'X_val' in globals() and 'y_train' in globals() and 'y_val' in globals(), 'Run the split cell first.'\n",
        "\n",
        "categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
        "\n",
        "clf = Pipeline(steps=[('prep', preprocess), ('lr', model)])\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "val_pred = clf.predict(X_val)\n",
        "acc = accuracy_score(y_val, val_pred)\n",
        "print('Validation accuracy:', round(float(acc), 2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline accuracy (full features): 0.6815068493150684\n",
            "Accuracy drop by feature: {'industry': -0.006849315068493178, 'employment_status': 0.0, 'lead_score': 0.006849315068493067}\n",
            "Smallest drop feature: industry\n"
          ]
        }
      ],
      "source": [
        "# Feature elimination: drop-one evaluation with same pipeline as Q4 (no rounding)\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "assert 'X_train' in globals() and 'X_val' in globals() and 'y_train' in globals() and 'y_val' in globals(), 'Run the split cell first.'\n",
        "\n",
        "# Build baseline pipeline on full features\n",
        "cat_full = X_train.select_dtypes(include=['object']).columns.tolist()\n",
        "prep_full = ColumnTransformer([\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_full)\n",
        "], remainder='passthrough')\n",
        "\n",
        "base_model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
        "base_clf = Pipeline([('prep', prep_full), ('lr', base_model)])\n",
        "base_clf.fit(X_train, y_train)\n",
        "base_acc = accuracy_score(y_val, base_clf.predict(X_val))\n",
        "print('Baseline accuracy (full features):', base_acc)\n",
        "\n",
        "# Evaluate drop-one for each feature present in X_train\n",
        "all_features = X_train.columns.tolist()\n",
        "acc_drop = {}\n",
        "\n",
        "for feat in all_features:\n",
        "    Xtr = X_train.drop(columns=[feat])\n",
        "    Xva = X_val.drop(columns=[feat])\n",
        "\n",
        "    cat_cols = Xtr.select_dtypes(include=['object']).columns.tolist()\n",
        "    prep = ColumnTransformer([\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)\n",
        "    ], remainder='passthrough')\n",
        "\n",
        "    clf = Pipeline([('prep', prep), ('lr', LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42))])\n",
        "    clf.fit(Xtr, y_train)\n",
        "    acc = accuracy_score(y_val, clf.predict(Xva))\n",
        "    acc_drop[feat] = base_acc - acc\n",
        "\n",
        "# Report drops for requested features\n",
        "requested = ['industry', 'employment_status', 'lead_score']\n",
        "report = {f: (acc_drop[f] if f in acc_drop else None) for f in requested}\n",
        "print('Accuracy drop by feature:', report)\n",
        "\n",
        "# Identify smallest drop among requested\n",
        "available = {k: v for k, v in report.items() if v is not None}\n",
        "if available:\n",
        "    least = min(available, key=lambda k: available[k])\n",
        "    print('Smallest drop feature:', least)\n",
        "else:\n",
        "    print('None of the requested features were found in X_train.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Regularized Logistic Regression: tune C over [0.01, 0.1, 1, 10, 100]\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "assert 'X_train' in globals() and 'X_val' in globals() and 'y_train' in globals() and 'y_val' in globals(), 'Run the split cell first.'\n",
        "\n",
        "categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "Cs = [0.01, 0.1, 1, 10, 100]\n",
        "acc_by_C = {}\n",
        "\n",
        "for C in Cs:\n",
        "    model = LogisticRegression(solver='liblinear', C=C, max_iter=1000, random_state=42)\n",
        "    clf = Pipeline(steps=[('prep', preprocess), ('lr', model)])\n",
        "    clf.fit(X_train, y_train)\n",
        "    val_pred = clf.predict(X_val)\n",
        "    acc = accuracy_score(y_val, val_pred)\n",
        "    acc_by_C[C] = round(float(acc), 3)\n",
        "\n",
        "print('Validation accuracy by C:', acc_by_C)\n",
        "best_C = max(acc_by_C, key=lambda k: acc_by_C[k])\n",
        "print('Best C:', best_C)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ml-zoomcamp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
